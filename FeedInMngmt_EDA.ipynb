{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "- **Net Downward Shortwave Radiation Flux [W/m^2]:** \n",
    "- **Wind Gust Surface [m/s]:** \n",
    "- **Planetary Boundary Layer Height [m]:** \n",
    "- **Mean Sea Level Pressure [pa]:** \n",
    "- **Potential Temperature Difference between 80m and 0m [K]:**\n",
    "- **Specific Humidity in 2m height [1]:** \n",
    "- **Specific Humidity Mean over first 30 hPa (~250m) [1]:** \n",
    "- **Relative Humidity in 925 hPa pressure level [%]:** \n",
    "- **Relative Humidity in 950 hPa pressure level [%]:**\n",
    "- **Net Sensible Heat Flux (conductive heat flux of the Earth surface to the atmosphere) [W/m^2]:**\n",
    "- **Temperature in 100m height [K]:** \n",
    "- **Temperature in 2m height [K]:** \n",
    "- **Temperature Mean over first 30 hPa (~250m) [K]:** \n",
    "- **Total Cloud Cover, low level clouds (0km - 2km height) [%]:** \n",
    "- **Total Cloud Cover, mid level clouds (2km - 7km height) [%]:** \n",
    "- **Wind Direction in 100 m height [°]:** \n",
    "- **Wind Direction in 10 m height [°]:** \n",
    "- **Wind Direction Mean over first 30 hPa (~250m) [°]:** \n",
    "- **Wind Direction in 925 hPa pressure level [°]:** \n",
    "- **Wind Speed in 100 m height [m/s]:** \n",
    "- **Wind Speed in 10 m height [m/s]:** \n",
    "- **Wind Speed Mean over first 30 hPa (~250m) [°]:** \n",
    "- **Wind Speed in 925 hPa pressure level [°]:** \n",
    "- **Date and 24 Hours divided in quarters [?]:** \n",
    "- **Azimuth angle of the sun [°]:** \n",
    "- **Elevation angle of the sun [°]:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Envoirment Set- Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load relevant Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the most important modules and setting the style for following plots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "\n",
    "# For Data Mining\n",
    "import os, glob\n",
    "\n",
    "# For Data Cleaning\n",
    "from datetime import datetime\n",
    "import missingno as msno\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seed for reproducability\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "sns.set(rc={'figure.figsize':(14,8)})\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Datasets & First Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GFS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the feature dataframe\n",
    "with open('./data/griddata_gfs_us_20180101_20190826_03_final.p', 'rb') as f:\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    df_features = u.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First and last observations:')\n",
    "pd.concat([df_features.head(2), df_features.tail(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns which contains symbols\n",
    "df_features = df_features.rename(columns={'r_pl925_%': 'r_pl925_perc','r_pl950_%': 'r_pl950_perc',\n",
    "                        'tcclow_sfc_%': 'tcclow_sfc_perc','tccmedium_sfc_%': 'tccmedium_sfc_perc'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadra Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the target dataframe\n",
    "with open('./data/obs_20180101_20190625_03_final_normed.p', 'rb') as f:\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    df_target = u.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First and last observations:')\n",
    "pd.concat([df_target.head(2), df_target.tail(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- **Timezones:** The timezones in the datasets are inconsistent\n",
    "- **Frequency**\n",
    "    - **feature data:** hourly frequency\n",
    "    - **target data:** 10minute frequency\n",
    "- **DataFrames**: target and feature data are in two seperate DataFrames\n",
    "- **Timeframe**\n",
    "    - **feature data:** 1st Jan 2018 06:00 to 26th Aug 2019 18:00\n",
    "    - **target data:** 1st Jan 2018 00:00  to 25th Jun 2019 01:50\n",
    "\n",
    "\n",
    "\n",
    "**Resulting Steps**: \n",
    "- timezones: creating consistency of timezones\n",
    "- frequency: resample feature data to have 10 minute frequency\n",
    "- DataFrames: Merging Data into one DataFrame\n",
    "- timeframes:reducing timeframes to have overlapping timeframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to same timezone\n",
    "df_features.index = df_features.index.tz_localize(None).to_series(keep_tz=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to target data to the same timezone as the feature dataframe\n",
    "df_target.index = df_target.index.tz_localize(None).to_series(keep_tz=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling to a time range of 10 minutes and interpolate between the hourly values\n",
    "df_features = df_features.resample('10min', axis='index').interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining both dataframes to have one to work in\n",
    "df = pd.concat([df_target,df_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[34:38,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations: **\n",
    "- `created_on`: frequency interpolation did not work for TimeStamp data, columns will be dropped anyway. \n",
    "- target dataframe (first three coloumns): harmonisation of overall timeframe needed\n",
    "- target data is uncontentious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage and Number of NaN-Values\n",
    "missing = pd.DataFrame(df.isnull().sum(),columns=['Number'])\n",
    "missing['Percentage'] = round(missing.Number/df.shape[0]*100,1)\n",
    "print()\n",
    "print('MISSING VALUES (absolut and in percent)')\n",
    "missing[missing.Number!=0].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Column \"created_on\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = [\"created_on\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeframe harmonisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_index_str = \"2019-06-25 01:50:00\"\n",
    "end_index = pd.to_datetime(end_index_str)\n",
    "\n",
    "start_index_str = \"2018-01-01 06:00:00\"\n",
    "start_index = pd.to_datetime(start_index_str)\n",
    "\n",
    "print('Start Index of united DataFrame: ', start_index)\n",
    "print('End Index of united DataFrame: ', end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[(df.index <= end_index) & (df.index >= start_index)]\n",
    "print('First and last observations:')\n",
    "pd.concat([df.head(2), df.tail(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with uncontentious target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target_losses_norm.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3: 'Thursday', 4: 'Friday', 5:'Saturday', 6:'Sunday'}\n",
    "\n",
    "df['index1'] = df.index\n",
    "df['Date'] = pd.to_datetime(df.index1.dt.date)\n",
    "df['year'] = df.index1.dt.year\n",
    "df['month'] = df.index1.dt.month\n",
    "df['day'] = df.index1.dt.day\n",
    "df['hour'] = df.index1.dt.hour\n",
    "df['minute'] = df.index1.dt.minute\n",
    "df['weekday'] = df.index1.dt.weekday.map(weekdays)\n",
    "df['day_hour_minute'] = df.index1.dt.weekday + df['hour']*1/24 + (df['minute']//10 * 1/144)\n",
    "\n",
    "\n",
    "df.drop(columns = [\"index1\"], inplace = True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting average hourly load profile observed over the entire period \n",
    "df.groupby('hour')['target_losses_norm'].mean().plot(figsize = (14,8))\n",
    "_ = plt.ylabel('Target Loss')\n",
    "_ = plt.ylim([0, max(df.groupby('hour')['target_losses_norm'].mean()) + 0.05])\n",
    "_ = plt.xticks(df['hour'].unique())\n",
    "_ = plt.title('Hourly Averaged Target Loss (Standard Day)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting 10 minute averaged target loss\n",
    "df.groupby('minute')['target_losses_norm'].mean().plot(figsize = (14,8))\n",
    "_ = plt.ylabel('Target Loss')\n",
    "_ = plt.ylim([0, max(df.groupby('minute')['target_losses_norm'].mean()) + 0.05])\n",
    "_ = plt.xticks(df['minute'].unique())\n",
    "_ = plt.title('10 Minute Averaged Target Loss (Standard Day)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating series objects with the different timestamps of a day and their corresponding mean values for target data\n",
    "day_hour_minute_means_loss = df.groupby('day_hour_minute')['target_losses_norm'].mean().reset_index()\n",
    "day_hour_minute_means_available = df.groupby('day_hour_minute')['power_available_mw_obsnorm'].mean().reset_index()\n",
    "day_hour_minute_means_cons = df.groupby('day_hour_minute')['power_mw_obsnorm'].mean().reset_index()\n",
    "\n",
    "# creating dictionaries to map the mean values to the according timestamps\n",
    "day_hour_minute_means_loss_dict = dict(zip(day_hour_minute_means_loss.day_hour_minute, day_hour_minute_means_loss.target_losses_norm))\n",
    "day_hour_minute_means_available_dict = dict(zip(day_hour_minute_means_available.day_hour_minute, day_hour_minute_means_available.power_available_mw_obsnorm))\n",
    "day_hour_minute_means_cons_dict = dict(zip(day_hour_minute_means_cons.day_hour_minute, day_hour_minute_means_cons.power_mw_obsnorm))\n",
    "\n",
    "# mapping the mean values to the according timestamps\n",
    "df[\"mean_losses\"] = df[\"day_hour_minute\"].map(day_hour_minute_means_loss_dict)\n",
    "df[\"mean_available\"] = df[\"day_hour_minute\"].map(day_hour_minute_means_available_dict)\n",
    "df[\"mean_cons\"] = df[\"day_hour_minute\"].map(day_hour_minute_means_cons_dict)\n",
    "\n",
    "# filling in the nan values in our target data with the mean value for the corresponding timestamp on the day\n",
    "df[\"target_losses_norm\"].fillna(df[\"mean_losses\"], inplace=True)\n",
    "df[\"power_available_mw_obsnorm\"].fillna(df[\"mean_losses\"], inplace=True)\n",
    "df[\"power_mw_obsnorm\"].fillna(df[\"mean_losses\"], inplace=True)\n",
    "\n",
    "# dropping the columns with the mean values again\n",
    "df.drop(columns = [\"mean_losses\", \"mean_available\", \"mean_cons\", \"day_hour_minute\"], inplace = True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage and Number of NaN-Values\n",
    "missing = pd.DataFrame(df.isnull().sum(),columns=['Number'])\n",
    "missing['Percentage'] = round(missing.Number/df.shape[0]*100,1)\n",
    "print()\n",
    "print('MISSING VALUES (absolut and in percent)')\n",
    "missing[missing.Number!=0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target_losses_norm.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"month\"] == 10][\"target_losses_norm\"].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using pivot table to create a dataframe having index as hours and columns as weekdays and each cell will contain the average\n",
    "#energy consumption for that particular hour of the weekday\n",
    "\n",
    "hour_weekday = df.pivot_table(values='target_losses_norm', index='hour', columns = 'weekday', aggfunc = 'mean')\n",
    "columns = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "hour_weekday = hour_weekday[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a heatmap with a colorbar; the colorbar shows the energy consumption in MWH\n",
    "_ = plt.figure(figsize=(12, 8))\n",
    "ax = sns.heatmap(hour_weekday.sort_index(ascending = False), cmap='viridis')\n",
    "#_ = plt.title('Average energy consumption in MWH for each hour of each weekday over the entire period')\n",
    "_ = ax.set_title(\"Average loss for each hour of each weekday averaged over dataset\", fontsize = 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using pivot table to create a dataframe having index as hours and columns as weekdays and each cell will contain the average\n",
    "#energy consumption for that particular hour of the weekday\n",
    "\n",
    "hour_weekday_consumed = df.pivot_table(values='power_mw_obsnorm', index='hour', columns = 'weekday', aggfunc = 'mean')\n",
    "columns = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "hour_weekday_consumed = hour_weekday[columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a heatmap with a colorbar; the colorbar shows the energy consumption in MWH\n",
    "_ = plt.figure(figsize=(12, 8))\n",
    "ax = sns.heatmap(hour_weekday_consumed.sort_index(ascending = False), cmap='viridis')\n",
    "#_ = plt.title('Average energy consumption in MWH for each hour of each weekday over the entire period')\n",
    "_ = ax.set_title(\"Average consumed power for each hour of each weekday averaged over dataset\", fontsize = 14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: \n",
    "- consumed power is not representative for the overall power consumption of the local grid, but rather for the amount of wind power fed into the grid (normed on the maximum available power = installed power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding column working day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the energy consumption in the grid might be correlated to the day being a working or non working day, a feature called working day is implemented. All the weekend days and the national holidays for Germany will be categorized as non-working days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#workalendar is a non-standard, single-use libary \n",
    "#workalendar is hence is loaded seperate\n",
    "try:\n",
    "    import workalendar\n",
    "    print(\"module 'workalendar' is installed.\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"module 'workalendar' will be installed and imported.\")\n",
    "    ! pip install workalendar\n",
    "    import workalendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workalendar.europe import Germany\n",
    "cal = Germany()\n",
    "df[\"working_day\"] = df[\"Date\"].apply(lambda x: cal.is_working_day(x))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for duplicate timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(df.index.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate timestamps in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding column season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the location of our wind farm is in northern Germany we will apply a filter on our dataset that separates the dates into two basic seasons (summer and winter). As the summer in northern Germany tends to be shorter, only the months from May to August will be declared summer months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_calc(month):\n",
    "    if month in [5,6,7,8]:\n",
    "        return 1 #\"summer\"\n",
    "    else:\n",
    "        return 0 #\"winter\"\n",
    "    \n",
    "df['summer'] = df.Date.dt.month.apply(season_calc)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph: Power Available vs Power fed into the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### DATA SELECTION ###### \n",
    "weekly_feedin = df['power_mw_obsnorm'].resample('W').mean()\n",
    "weekly_available = df['power_available_mw_obsnorm'].resample('W').mean()\n",
    "weekly_loss = df['target_losses_norm'].resample('W').mean()\n",
    "###### DATA PLOTTING ###### \n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "weekly_feedin.plot(label='Power fed into grid',color='olive', lw=3)\n",
    "weekly_available.plot(label='Power available',color='goldenrod', lw=3)\n",
    "weekly_loss.plot(label='Feed-in Managment',color='grey', alpha=0.3,  lw=1)\n",
    "\n",
    "### Moving Mean\n",
    "#moving_mean = df['power_mw_obsnorm'].resample('W').mean().rolling(4).mean()\n",
    "#moving_mean.plot(label='Power available',color='green')\n",
    "######\n",
    "\n",
    "\n",
    "###### PLOT SETTINGS #######\n",
    "plt.ylabel('normalized', fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.title('Weekly available power and power actually fed into grid', fontsize=20)\n",
    "plt.legend(loc='upper right', fontsize=14)\n",
    "\n",
    "### CLEANING WORKSPACE ###\n",
    "del weekly_feedin, weekly_available, weekly_loss\n",
    "\n",
    "\n",
    "###### OUTPUT #######\n",
    "plt.savefig('figures/available_fedin.png', bbox_inches='tight', transparent=True)\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** At any given week, the available power is greater than the power fed into the grid (used power). Hence, at any given week curtailment of power fed into the system is observable (feed-in managment / EinsMan Events). In an ideal week, the power available nearly equals the power used/fed into the grid - a balanced energy grid with very little energy being lost (read: not created, e.g., due to pitching of wind turbine blades). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph: Influence of Wind on the Feed-In Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### DATA SELECTION ###### \n",
    "weekly_wsp = df['wsp_100m_ms'].resample('W').mean()\n",
    "weekly_wsp = (weekly_wsp-weekly_wsp.min())/(weekly_wsp.max()-weekly_wsp.min())\n",
    "weekly_loss = df['target_losses_norm'].resample('W').mean()\n",
    "###### DATA PLOTTING ###### \n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "weekly_wsp.plot(label='wsp_100m_ms',color='olive', lw=3)\n",
    "weekly_loss.plot(label='Feed-in Managment',color='sandybrown',  lw=3)\n",
    "\n",
    "\n",
    "###### PLOT SETTINGS #######\n",
    "plt.ylabel('normalized Wind | Feed-In Mgmt', fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.title('Weekly available power and power actually fed into grid', fontsize=20)\n",
    "plt.legend(loc='upper right', fontsize=14)\n",
    "\n",
    "### CLEANING WORKSPACE ###\n",
    "del weekly_wsp, weekly_loss\n",
    "\n",
    "\n",
    "###### OUTPUT #######\n",
    "plt.savefig('figures/wind_FeedIn.png', bbox_inches='tight', transparent=True)\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** On a weekly bases, only little correlation between high windspeeds and Feed-In Managment can be observered. `-->` Looking at a smaller timespan at higher resultion in the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### DATA SELECTION ###### \n",
    "start_index = pd.to_datetime(\"2019-01-01 00:00:00\")\n",
    "end_index = pd.to_datetime(\"2019-04-01 00:00:00\")\n",
    "#reduced timeframe\n",
    "df_reduced = df[(df.index <= end_index) & (df.index >= start_index)]\n",
    "#windspeed resampling and normalisation\n",
    "weekly_wsp_100 = df_reduced['wsp_100m_ms'].resample('D').mean()\n",
    "weekly_wsp_100 = (weekly_wsp_100-weekly_wsp_100.min())/(weekly_wsp_100.max()-weekly_wsp_100.min())\n",
    "weekly_wsp_10 = df_reduced['wsp_10m_ms'].resample('D').mean()\n",
    "weekly_wsp_10 = (weekly_wsp_10-weekly_wsp_10.min())/(weekly_wsp_10.max()-weekly_wsp_10.min())\n",
    "#loss resampling\n",
    "weekly_loss = df_reduced['target_losses_norm'].resample('D').mean()\n",
    "###### DATA PLOTTING ###### \n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "weekly_wsp_100.plot(label='wsp_100m_ms',color='sandybrown', lw=2)\n",
    "weekly_wsp_10.plot(label='wsp_10m_ms',color='sienna', lw=2)\n",
    "weekly_loss.plot(label='Feed-In Managment',color='olive',  lw=3)\n",
    "\n",
    "\n",
    "###### PLOT SETTINGS #######\n",
    "plt.ylabel('normalized Wind | Feed-In Mgmt', fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.title('Influence of Wind on Feed-In Managment [detailed view]', fontsize=20)\n",
    "plt.legend(loc='upper right', fontsize=14)\n",
    "\n",
    "### CLEANING WORKSPACE ###\n",
    "del weekly_wsp_100, weekly_wsp_10, weekly_loss\n",
    "\n",
    "\n",
    "###### OUTPUT #######\n",
    "plt.savefig('figures/wind_FeedIn_detail.png', bbox_inches='tight', transparent=True)\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** \n",
    "- As a rule-of-thumb, windspeeds above a certain value (e.g., 0.4) stimulate the Feed-In Management. \n",
    "- windspeed at 100m above ground is highly corrolated to windspeed at 10m above ground. `wsp_10m_ms` is the windspeed at turbine height, hence wsp_10m_ms will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
